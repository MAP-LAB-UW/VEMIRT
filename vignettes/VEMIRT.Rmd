---
title: Variational Estimation for Multidimensional Item Response Theory Using VEMIRT
output:
  rmarkdown::html_vignette:
    toc: true
bibliography: references.bib
csl: apa.csl
link-citations: true
vignette: >
  %\VignetteIndexEntry{VEMIRT}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!-- Run knitr::knit("VEMIRT.Rmd.orig", output = "VEMIRT.Rmd") to prebuild the vignette -->



# Introduction
In this tutorial, we illustrate how to conduct multidimensional item response theory (MIRT) analysis of multidimensional two parameter logistic (M2PL) and multidimensional three parameter logistic (M3PL) models, and differential item functioning (DIF) analysis of M2PL models using the `VEMIRT` package in `R`, which can be installed with

``` r
if (!require(devtools)) install.packages("devtools")
devtools::install_github("MAP-LAB-UW/VEMIRT", build_vignettes = T)
torch::install_torch()
```
The package requires a C++ compiler to work properly, and users are referred to https://github.com/MAP-LAB-UW/VEMIRT for more information.

``` r
library(VEMIRT)
```
Most functions are based on the Gaussian variational expectation-maximization (GVEM) algorithm, which is applicable for high-dimensional latent traits.


# Data Input
Data required for analysis are summarized below:

| Analysis | Item Responses | Loading Indicator | Group Membership |
| :-: | :-: | :-: | :-: |
| Exploratory Factor Analysis | $\checkmark$ |
| Confirmatory Factor Analysis | $\checkmark$ | $\checkmark$ |
| Differential Item Functioning | $\checkmark$ | $\checkmark$ | $\checkmark$ |

Here we take dataset `D2PL_data` as an example. This simulated dataset is for DIF 2PL analysis.
Responses should be an $N$ by $J$ binary matrix, where $N$ and $J$ are the numbers of respondents and items respectively. Currently, all DIF functions and `C2PL_iw2` allow responses to have missing data, which should be coded as `NA`. In this example, there are $N=1500$ respondents and $J=20$ items.

``` r
head(D2PL_data$data)
#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19]
#> [1,]    0    1    0    1    1    0    1    0    0     0     1     0     0     1     0     1     1     1     1
#> [2,]    0    0    0    0    1    0    0    0    0     0     0     0     0     1     0     0     0     0     0
#> [3,]    0    0    0    0    0    0    1    0    0     0     0     0     0     0     0     0     1     0     1
#> [4,]    0    0    1    0    1    0    0    0    0     1     1     1     1     1     1     1     0     1     0
#> [5,]    0    0    0    0    0    0    0    0    0     1     0     0     0     0     0     1     0     0     0
#> [6,]    1    1    1    1    1    0    1    1    1     1     1     1     1     1     1     1     1     1     0
#>      [,20]
#> [1,]     0
#> [2,]     0
#> [3,]     0
#> [4,]     0
#> [5,]     0
#> [6,]     1
```

CFA and DIF rely on a $J$ by $D$ binary loading indicator matrix specifying latent dimensions each item loads on, where $D$ is the number of latent dimensions. The latent traits have $D=2$ dimensions here.

``` r
D2PL_data$model
#>       [,1] [,2]
#>  [1,]    1    0
#>  [2,]    0    1
#>  [3,]    1    0
#>  [4,]    0    1
#>  [5,]    1    0
#>  [6,]    0    1
#>  [7,]    1    0
#>  [8,]    0    1
#>  [9,]    1    0
#> [10,]    0    1
#> [11,]    1    0
#> [12,]    0    1
#> [13,]    1    0
#> [14,]    0    1
#> [15,]    1    0
#> [16,]    0    1
#> [17,]    1    0
#> [18,]    0    1
#> [19,]    1    0
#> [20,]    0    1
```

DIF analysis additionally needs a group membership vector of length $N$, whose elements are integers from $1$ to $G$, where $G$ is the number of groups. There are $G=3$ groups in this example.

``` r
table(D2PL_data$group)
#> 
#>   1   2   3 
#> 500 500 500
```


# Data Output
All the functions output estimates of item parameters and some other related parameters. In addition, `C2PL_gvem`, `C2PL_bs` and `C2PL_iw2` are able to provide the standard errors of item parameter estimates.


# Exploratory Factor Analysis
## Parallel Analysis
Parallel analysis can be conducted to determine the number of factors. Users can specify the number of simulated datasets, which takes `n.iter = 10` by default.

``` r
pa_poly(D2PL_data$data, n.iter = 5)
#> Parallel analysis suggests that the number of factors =  2
```

![](figure/unnamed-chunk-6-1.png)


## M2PL Model
`VEMIRT` provides the following functions to conduct EFA for the M2PL model:

| Function | Description |
| :-: | :-: |
| `E2PL_gvem_rot` | GVEM with post-hoc rotation |
| `E2PL_gvem_lasso` | GVEM with lasso penalty |
| `E2PL_gvem_adaptlasso` | GVEM with adaptive lasso penalty |
| `E2PL_iw` | Importance sampling for GVEM |

Currently these functions do not estimate the standard errors of item parameters. The following examples use two simulated datasets, `E2PL_data_C1` and `E2PL_data_C2`, both having $N=1000$ respondents, $J=30$ items and $D=3$ dimensions, but items load on different dimensions.

`E2PL_gvem_rot` needs the item responses and the number of factors (`domain`), and applies the promax rotation (`rot = "Promax"`) by default. Another choice is `rot = "cfQ"`, which performs the CF-Quartimax rotation.

``` r
E2PL_gvem_rot(E2PL_data_C1$data, domain = 3)
#>          a1      a2       a3       b
#> 1   0.10594  1.5952 -0.10845  1.7290
#> 2   1.91968  0.0721 -0.05631 -1.3236
#> 3   0.07972 -0.0394  1.56129 -1.2989
#> 4   0.19942  1.8727 -0.05273 -0.8808
#> 5   1.73784  0.0441 -0.11143 -0.8111
#> 6  -0.05117  0.0230  1.63743  0.1912
#> 7  -0.05174  1.3707 -0.00506 -0.9726
#> 8   1.52949  0.1766 -0.03231  0.4670
#> 9  -0.01529  0.0186  1.42425  1.1823
#> 10  0.04563  1.4053  0.07149  1.0613
#> 11  1.64560 -0.0849  0.06168  0.8636
#> 12  0.08978  0.0130  1.65967 -0.6542
#> 13  0.10412  1.8613  0.09076  0.0380
#> 14  1.42716 -0.1292  0.18454  1.5696
#> 15  0.03152  0.0564  1.78979 -1.3211
#> 16 -0.03327  1.7001  0.17390 -1.9581
#> 17  1.84224 -0.0486 -0.00188  0.5343
#> 18 -0.00405 -0.0918  1.60662  0.5123
#> 19 -0.19546  1.8230 -0.07211  0.1182
#> 20  1.34056 -0.0278  0.07707  0.3125
#> 21  0.14051 -0.0800  1.71112  0.1102
#> 22 -0.06479  1.9194 -0.02918 -0.6766
#> 23  1.55836  0.2900  0.05540  1.0387
#> 24 -0.20321  0.1247  1.59738  0.0535
#> 25  0.04718  1.4758  0.06536  0.3251
#> 26  1.75190 -0.0335  0.07172 -0.0327
#> 27  0.11757  0.0217  1.65739  0.5990
#> 28  0.00903  1.7688  0.05774 -1.5098
#> 29  1.65340 -0.0803  0.04002 -1.0095
#> 30 -0.20175  0.0796  1.52666 -0.1739
```

Both `E2PL_gvem_lasso` and `E2PL_gvem_adaptlasso` need item responses, constraint setting (`constrain`), and a binary matrix specifying constraints on the sub-matrix of the factor loading structure (`indic`). `constrain` should be either `"C1"` or `"C2"` to ensure identifiability. Under `"C1"`, a $D\times D$ sub-matrix of `indic` should be an identity matrix, indicating that each of these $D$ items loads solely on one factor. Notice that the first 3 rows of `E2PL_data_C1$model` form an identity matrix.

``` r
E2PL_data_C1$model
#>       [,1] [,2] [,3]
#>  [1,]    1    0    0
#>  [2,]    0    1    0
#>  [3,]    0    0    1
#>  [4,]    1    1    1
#>  [5,]    1    1    1
#>  [6,]    1    1    1
#>  [7,]    1    1    1
#>  [8,]    1    1    1
#>  [9,]    1    1    1
#> [10,]    1    1    1
#> [11,]    1    1    1
#> [12,]    1    1    1
#> [13,]    1    1    1
#> [14,]    1    1    1
#> [15,]    1    1    1
#> [16,]    1    1    1
#> [17,]    1    1    1
#> [18,]    1    1    1
#> [19,]    1    1    1
#> [20,]    1    1    1
#> [21,]    1    1    1
#> [22,]    1    1    1
#> [23,]    1    1    1
#> [24,]    1    1    1
#> [25,]    1    1    1
#> [26,]    1    1    1
#> [27,]    1    1    1
#> [28,]    1    1    1
#> [29,]    1    1    1
#> [30,]    1    1    1
```
Under `"C2"`, a $D\times D$ sub-matrix of `indic` should be a lower triangular matrix whose diagonal elements are all one, indicating that each of these $D$ items loads on one factor and potentially other factors as well; non-zero elements other than the diagonal are penalized. For identification under `"C2"`, another argument `non_pen` should be provided, which specifies an anchor item that loads on all the factors. In the following example, the first 2 rows and any other row form such a lower triangular matrix, so `non_pen` can take any integer from $3$ to $30$.

``` r
E2PL_data_C2$model
#>       [,1] [,2] [,3]
#>  [1,]    1    0    0
#>  [2,]    1    1    0
#>  [3,]    1    1    1
#>  [4,]    1    1    1
#>  [5,]    1    1    1
#>  [6,]    1    1    1
#>  [7,]    1    1    1
#>  [8,]    1    1    1
#>  [9,]    1    1    1
#> [10,]    1    1    1
#> [11,]    1    1    1
#> [12,]    1    1    1
#> [13,]    1    1    1
#> [14,]    1    1    1
#> [15,]    1    1    1
#> [16,]    1    1    1
#> [17,]    1    1    1
#> [18,]    1    1    1
#> [19,]    1    1    1
#> [20,]    1    1    1
#> [21,]    1    1    1
#> [22,]    1    1    1
#> [23,]    1    1    1
#> [24,]    1    1    1
#> [25,]    1    1    1
#> [26,]    1    1    1
#> [27,]    1    1    1
#> [28,]    1    1    1
#> [29,]    1    1    1
#> [30,]    1    1    1
```
`E2PL_gvem_adaptlasso` needs an additional tuning parameter, which takes `gamma = 2` by default. Users are referred to @cho2024 for algorithmic details.

``` r
result <- with(E2PL_data_C1, E2PL_gvem_lasso(data, model, constrain = "C1"))
result
#>       a1     a2   a3       b
#> 1  1.572  0.000 0.00  1.7198
#> 2  0.000  1.923 0.00 -1.3243
#> 3  0.000  0.000 1.58 -1.2981
#> 4  1.928  0.000 0.00 -0.8756
#> 5  0.000  1.696 0.00 -0.8114
#> 6  0.000  0.000 1.62  0.1907
#> 7  1.337  0.000 0.00 -0.9712
#> 8  0.000  1.589 0.00  0.4653
#> 9  0.000  0.000 1.42  1.1816
#> 10 1.473  0.000 0.00  1.0629
#> 11 0.000  1.639 0.00  0.8623
#> 12 0.000  0.000 1.71 -0.6524
#> 13 1.964  0.000 0.00  0.0408
#> 14 0.000  1.459 0.00  1.5628
#> 15 0.000  0.000 1.84 -1.3199
#> 16 1.776  0.000 0.00 -1.9514
#> 17 0.000  1.807 0.00  0.5304
#> 18 0.000  0.000 1.55  0.5113
#> 19 1.812 -0.277 0.00  0.1169
#> 20 0.000  1.367 0.00  0.3120
#> 21 0.000  0.000 1.73  0.1104
#> 22 1.861  0.000 0.00 -0.6754
#> 23 0.322  1.560 0.00  1.0364
#> 24 0.000  0.000 1.54  0.0525
#> 25 1.544  0.000 0.00  0.3269
#> 26 0.000  1.775 0.00 -0.0335
#> 27 0.000  0.000 1.74  0.6011
#> 28 1.803  0.000 0.00 -1.5078
#> 29 0.000  1.633 0.00 -1.0087
#> 30 0.000 -0.204 1.57 -0.1730
with(E2PL_data_C2, E2PL_gvem_adaptlasso(data, model, constrain = "C2", non_pen = 3))
#>         a1    a2   a3       b
#> 1   1.6019 0.000 0.00  1.7409
#> 2   0.4230 2.419 0.00 -1.1530
#> 3   0.0000 2.653 1.00 -1.2417
#> 4   1.9203 0.000 0.00 -0.8737
#> 5  -1.2687 2.575 0.00 -0.8181
#> 6  -0.1981 0.993 1.29  0.1904
#> 7   1.3404 0.000 0.00 -0.9736
#> 8  -0.8955 2.214 0.00  0.4638
#> 9  -0.2045 0.912 1.12  1.1810
#> 10  1.5077 0.000 0.00  1.0795
#> 11 -1.2717 2.493 0.00  0.8623
#> 12 -0.3468 1.254 1.32 -0.6612
#> 13  2.0240 0.000 0.00  0.0465
#> 14 -1.1050 2.213 0.00  1.5660
#> 15 -0.2273 1.169 1.39 -1.3111
#> 16  1.8359 0.000 0.00 -1.9954
#> 17 -1.3386 2.723 0.00  0.5314
#> 18 -0.3741 1.040 1.27  0.5081
#> 19  1.6743 0.000 0.00  0.1161
#> 20 -0.9951 2.069 0.00  0.3137
#> 21 -0.4841 1.336 1.34  0.1086
#> 22  1.8836 0.000 0.00 -0.6795
#> 23 -0.7954 2.335 0.00  1.0436
#> 24  0.0000 0.736 1.25  0.0521
#> 25  1.5476 0.000 0.00  0.3304
#> 26 -1.2496 2.608 0.00 -0.0336
#> 27 -0.3240 1.235 1.30  0.5954
#> 28  1.8113 0.000 0.00 -1.5157
#> 29 -1.2838 2.500 0.00 -1.0119
#> 30 -0.0444 0.706 1.23 -0.1758
```

GVEM is known to produce biased estimates for discrimination parameters, and `E2PL_iw` helps reduce the bias through importance sampling [@ma2024].

``` r
E2PL_iw(E2PL_data_C1$data, result)
#>      a1     a2   a3       b
#> 1  1.69  0.000 0.00  1.8353
#> 2  0.00  2.050 0.00 -1.4302
#> 3  0.00  0.000 1.71 -1.4091
#> 4  2.05  0.000 0.00 -0.9806
#> 5  0.00  1.822 0.00 -0.9043
#> 6  0.00  0.000 1.74  0.2367
#> 7  1.46  0.000 0.00 -1.0768
#> 8  0.00  1.712 0.00  0.5549
#> 9  0.00  0.000 1.54  1.2904
#> 10 1.59  0.000 0.00  1.1686
#> 11 0.00  1.763 0.00  0.9668
#> 12 0.00  0.000 1.84 -0.7436
#> 13 2.09  0.000 0.00 -0.0156
#> 14 0.00  1.580 0.00  1.6785
#> 15 0.00  0.000 1.96 -1.4296
#> 16 1.90  0.000 0.00 -2.0679
#> 17 0.00  1.933 0.00  0.6236
#> 18 0.00  0.000 1.68  0.5942
#> 19 1.94 -0.191 0.00  0.0856
#> 20 0.00  1.486 0.00  0.3882
#> 21 0.00  0.000 1.85  0.1406
#> 22 1.99  0.000 0.00 -0.7758
#> 23 0.42  1.683 0.00  1.1470
#> 24 0.00  0.000 1.66  0.0641
#> 25 1.67  0.000 0.00  0.3795
#> 26 0.00  1.900 0.00  0.0194
#> 27 0.00  0.000 1.86  0.6879
#> 28 1.93  0.000 0.00 -1.6212
#> 29 0.00  1.757 0.00 -1.1104
#> 30 0.00 -0.117 1.69 -0.2293
```


## M3PL Model
`VEMIRT` provides the following functions to conduct EFA for the M3PL model:

| Function | Description |
| :-: | :-: |
| `E3PL_sgvem_rot` | Stochastic GVEM with post-hoc rotation |
| `E3PL_sgvem_lasso` | Stochastic GVEM with lasso penalty |
| `E3PL_sgvem_adaptlasso` | Stochastic GVEM with adaptive lasso penalty |

The following examples use two simulated datasets, `E3PL_data_C1` and `E3PL_data_C2`, both having $N=1000$ respondents, $J=30$ items and $D=3$ dimensions, but items load on different dimensions.

The usage of these functions is similar to those for M2PL models, but some additional arguments are required: the size of the subsample for each iteration (`samp = 50` by default), the forget rate for the stochastic algorithm (`forgetrate = 0.51` by default), the mean and the variance of the normal distribution as a prior for item difficulty parameters (`mu_b` and `sigma2_b`), the $\alpha$ and $\beta$ parameters of the beta distribution as a prior for guessing parameters (`Alpha` and `Beta`). Still, `E3PL_sgvem_adaptlasso` needs a tuning parameter, which takes `gamma = 2` by default. Users are referred to @cho2024 for algorithmic details. In the following examples, the priors for difficulty parameters and guessing parameters are $N(0,2^2)$ and $\beta(10,40)$ respectively.

``` r
with(E3PL_data_C1, E3PL_sgvem_adaptlasso(data, model, mu_b = 0, sigma2_b = 4, Alpha = 10, Beta = 40, constrain = "C1"))
#>       a1    a2    a3       b     c
#> 1  0.794 0.000 0.000  0.1891 0.185
#> 2  0.000 1.290 0.000 -1.2229 0.188
#> 3  0.000 0.000 1.374 -1.2121 0.187
#> 4  1.755 0.000 0.000 -0.6975 0.181
#> 5  0.000 1.382 0.000 -1.0080 0.187
#> 6  0.000 0.000 1.311  0.0494 0.182
#> 7  1.480 0.000 0.000 -0.9579 0.184
#> 8  0.000 1.148 0.000  0.2136 0.182
#> 9  0.000 0.000 0.903  0.7030 0.180
#> 10 1.152 0.000 0.000  0.6787 0.177
#> 11 0.000 1.281 0.000  0.7833 0.172
#> 12 0.000 0.000 1.170 -0.8145 0.187
#> 13 1.865 0.000 0.000  0.4726 0.162
#> 14 0.000 0.965 0.000  1.0470 0.174
#> 15 0.000 0.000 1.239 -1.0900 0.188
#> 16 1.437 0.000 0.000 -1.5191 0.188
#> 17 0.000 1.480 0.000  0.1825 0.176
#> 18 0.000 0.000 1.080  0.4050 0.180
#> 19 1.445 0.000 0.000 -0.1578 0.179
#> 20 0.000 1.110 0.000  0.2268 0.182
#> 21 0.000 0.000 1.343  0.1158 0.176
#> 22 1.728 0.000 0.000 -0.3874 0.176
#> 23 0.000 1.195 0.000  0.9668 0.175
#> 24 0.000 0.000 1.053 -0.5243 0.186
#> 25 1.393 0.000 0.000  0.2389 0.176
#> 26 0.000 1.294 0.000  0.2698 0.180
#> 27 0.000 0.000 1.095  0.0491 0.182
#> 28 1.403 0.000 0.000 -1.2013 0.187
#> 29 0.000 1.182 0.000 -0.8919 0.186
#> 30 0.000 0.000 1.070 -0.2430 0.184
with(E3PL_data_C2, E3PL_sgvem_lasso(data, model, mu_b = 0, sigma2_b = 4, Alpha = 10, Beta = 40, constrain = "C2", non_pen = 3))
#>        a1      a2     a3       b     c
#> 1   0.651  0.0000 0.0000  0.1974 0.183
#> 2   0.000  1.4557 0.0000 -1.2644 0.183
#> 3   0.000  0.0000 1.6780 -1.0679 0.181
#> 4   1.134  0.5253 0.0000 -0.7284 0.182
#> 5  -0.213  1.2971 0.0000 -0.9946 0.185
#> 6   0.000 -0.3714 1.5183  0.0309 0.179
#> 7   0.911  0.3709 0.1708 -1.0041 0.184
#> 8  -0.396  1.2365 0.0000  0.1655 0.177
#> 9   0.000  0.0000 0.8501  0.7159 0.177
#> 10  0.817  0.2930 0.0000  0.6894 0.175
#> 11 -0.275  1.3122 0.0000  0.7671 0.168
#> 12  0.000  0.0000 1.1435 -0.8549 0.186
#> 13  1.114  0.4229 0.3675  0.5424 0.162
#> 14  0.110  0.7856 0.0000  1.0758 0.172
#> 15  0.000 -0.3320 1.4868 -1.1485 0.186
#> 16  0.843  0.4697 0.1483 -1.5600 0.187
#> 17 -0.296  1.3820 0.0000  0.1664 0.173
#> 18  0.000 -0.0138 0.9824  0.4223 0.180
#> 19  0.933  0.4708 0.0000 -0.1704 0.179
#> 20 -0.327  1.1643 0.0000  0.1555 0.179
#> 21  0.000 -0.3426 1.5295  0.1104 0.173
#> 22  1.109  0.5069 0.0497 -0.4049 0.177
#> 23 -0.200  1.1294 0.0000  0.9396 0.172
#> 24  0.000  0.0000 0.9778 -0.5629 0.187
#> 25  1.007  0.3174 0.0000  0.2351 0.175
#> 26 -0.347  1.3133 0.0000  0.1969 0.177
#> 27  0.000 -0.1086 1.1230  0.0685 0.182
#> 28  1.029  0.2085 0.0863 -1.2286 0.186
#> 29 -0.138  1.0991 0.0000 -0.9710 0.186
#> 30  0.000 -0.3731 1.2805 -0.2715 0.182
```


# Confirmatory Factor Analysis
## M2PL Model
`VEMIRT` provides the following functions to conduct CFA for the M2PL model:

| Function | Description |
| :-: | :-: |
| `C2PL_gvem` | GVEM |
| `C2PL_bs` | Bootstrap for GVEM |
| `C2PL_iw` | Importance sampling for GVEM |
| `C2PL_iw2` | IW-GVEM |

A binary loading indicator matrix needs to be provided for CFA. `C2PL_gvem` can produce biased estimates while the other two functions help reduce the bias. Also, `C2PL_gvem`, `C2PL_bs` and `C2PL_iw2` are able to provide the standard errors of item parameters. `C2PL_iw` and `C2PL_iw2` apply almost the same algorithm but have different implementations. More specifically, `C2PL_iw2` calls `D2PL_gvem` for estimation, and unlike `C2PL_iw` which uses stochastic gradient descent by resampling posteriors of latent traits in each iteration, `C2PL_iw2` only samples posteriors once and tends to be less stable but more accurate. Users are referred to @cho2021 for `C2PL_gvem` and @ma2024 for `C2PL_iw` and `C2PL_iw2`.

The following examples use a simulated dataset, `C2PL_data`, which has $N=1000$ respondents, $J=20$ items and $D=2$ dimensions.

``` r
result <- with(C2PL_data, C2PL_gvem(data, model))
result
#>      a1   a2       b
#> 1  1.91 0.00  0.8789
#> 2  0.00 1.71 -2.1325
#> 3  1.85 0.00  0.4760
#> 4  0.00 2.03 -1.2061
#> 5  1.89 0.00 -0.6088
#> 6  0.00 1.57  1.1850
#> 7  1.46 0.00 -1.8576
#> 8  0.00 1.79 -0.3041
#> 9  1.40 0.00  0.3218
#> 10 0.00 1.49 -0.4536
#> 11 1.89 0.00 -0.3825
#> 12 0.00 1.71 -0.1322
#> 13 1.36 0.00  0.5896
#> 14 0.00 1.43 -2.1947
#> 15 1.80 0.00  0.0357
#> 16 0.00 1.51  0.9673
#> 17 1.97 0.00  0.1262
#> 18 0.00 1.68  0.5347
#> 19 1.81 0.00 -0.7190
#> 20 0.00 1.62 -1.4178
C2PL_bs(result)
#>      a1   a2       b
#> 1  2.22 0.00  1.0157
#> 2  0.00 2.04 -2.3936
#> 3  2.22 0.00  0.5223
#> 4  0.00 2.34 -1.3797
#> 5  2.23 0.00 -0.6784
#> 6  0.00 1.74  1.2075
#> 7  1.67 0.00 -1.9693
#> 8  0.00 2.05 -0.3203
#> 9  1.55 0.00  0.3333
#> 10 0.00 1.74 -0.5015
#> 11 2.24 0.00 -0.4104
#> 12 0.00 1.99 -0.1192
#> 13 1.46 0.00  0.6153
#> 14 0.00 1.69 -2.3542
#> 15 2.11 0.00  0.0356
#> 16 0.00 1.71  0.9975
#> 17 2.24 0.00  0.1562
#> 18 0.00 1.81  0.4763
#> 19 2.06 0.00 -0.7782
#> 20 0.00 1.84 -1.5228
C2PL_iw(C2PL_data$data, result)
#>      a1   a2       b
#> 1  2.16 0.00  1.0352
#> 2  0.00 1.96 -2.3604
#> 3  2.09 0.00  0.5737
#> 4  0.00 2.28 -1.4103
#> 5  2.14 0.00 -0.7289
#> 6  0.00 1.80  1.3748
#> 7  1.70 0.00 -2.0733
#> 8  0.00 2.03 -0.4369
#> 9  1.63 0.00  0.3934
#> 10 0.00 1.72 -0.5925
#> 11 2.14 0.00 -0.4616
#> 12 0.00 1.95 -0.2265
#> 13 1.58 0.00  0.7169
#> 14 0.00 1.67 -2.4219
#> 15 2.05 0.00  0.0132
#> 16 0.00 1.74  1.1379
#> 17 2.22 0.00  0.1296
#> 18 0.00 1.92  0.6167
#> 19 2.06 0.00 -0.8566
#> 20 0.00 1.86 -1.6292
with(C2PL_data, C2PL_iw2(data, model, SE.level = 10))
#>            1      2     3      4      5     6      7      8      9      10     11      12     13     14     15
#> a1     2.328  0.000 2.233  0.000  2.307 0.000  1.662  0.000 1.5332  0.0000  2.307  0.0000 1.4889  0.000 2.1671
#> a2     0.000  2.114 0.000  2.586  0.000 1.748  0.000  2.133 0.0000  1.6787  0.000  2.0354 0.0000  1.700 0.0000
#> b      0.954 -2.390 0.507 -1.415 -0.686 1.231 -1.966 -0.359 0.3214 -0.4958 -0.437 -0.1682 0.5991 -2.378 0.0256
#> SE(a1) 0.189  0.000 0.185  0.000  0.186 0.000  0.157  0.000 0.1251  0.0000  0.189  0.0000 0.1256  0.000 0.1678
#> SE(a2) 0.000  0.191 0.000  0.228  0.000 0.155  0.000  0.179 0.0000  0.1341  0.000  0.1601 0.0000  0.157 0.0000
#> SE(b)  0.119  0.172 0.108  0.141  0.110 0.109  0.138  0.105 0.0864  0.0904  0.109  0.0977 0.0886  0.150 0.0999
#>           16    17     18     19    20
#> a1     0.000 2.431 0.0000  2.172  0.00
#> a2     1.655 0.000 1.9226  0.000  1.93
#> b      0.991 0.123 0.5467 -0.796 -1.56
#> SE(a1) 0.000 0.194 0.0000  0.177  0.00
#> SE(a2) 0.149 0.000 0.1587  0.000  0.17
#> SE(b)  0.104 0.108 0.0975  0.109  0.13
```

## M3PL Model
`C3PL_sgvem` conducts CFA for M3PL models. Its usage is similar to that of `E3PL_sgvem_*` except that a binary loading indicator matrix is needed additionally. Users are referred to @cho2021 for algorithmic details.

The following example uses a simulated dataset, `C3PL_data`, which has $N=2000$ respondents, $J=20$ items and $D=2$ dimensions. The priors for difficulty parameters and guessing parameters are chosen to be $N(0,2^2)$ and $\beta(10,40)$ respectively.

``` r
with(C3PL_data, C3PL_sgvem(data, model, mu_b = 0, sigma2_b = 4, Alpha = 10, Beta = 40))
#>      a1    a2       b     c
#> 1  1.16 0.000 -0.1390 0.185
#> 2  0.00 1.451  1.5487 0.151
#> 3  1.73 0.000 -0.5207 0.179
#> 4  0.00 1.494 -1.5277 0.187
#> 5  1.38 0.000 -1.6949 0.188
#> 6  0.00 1.336  1.5977 0.156
#> 7  1.26 0.000 -0.6656 0.186
#> 8  0.00 1.385  0.0985 0.178
#> 9  1.66 0.000  0.6166 0.172
#> 10 0.00 1.390 -0.1053 0.182
#> 11 1.19 0.000  0.4890 0.177
#> 12 0.00 0.924  1.3498 0.173
#> 13 1.42 0.000  1.4915 0.157
#> 14 0.00 1.165  0.8381 0.174
#> 15 1.20 0.000 -0.0716 0.181
#> 16 0.00 1.498  0.6667 0.168
#> 17 1.40 0.000 -0.4564 0.182
#> 18 0.00 1.676 -1.2551 0.183
#> 19 1.01 0.000  1.0441 0.173
#> 20 0.00 1.133 -0.7544 0.186
```


# Differential Item Functioning
`VEMIRT` provides the following functions to detect DIF for the M2PL model:

| Function | Description |
| :-: | :-: |
| `D2PL_lrt` | Likelihood ratio test |
| `D2PL_em` | EM with lasso penalty |
| `D2PL_pair_em` | EM with group pairwise truncated lasso penalty |
| `D2PL_gvem` | GVEM with lasso penalty |

Currently `D2PL_pair_em` supports unidimensional latent trait only, but it is strongly recommended when $D=1$ because it produces the most accurate estimates and allows comparison between every pair of groups. `D2PL_pair_em` requires item responses and group membership as input. It does not require the loading indicator and assumes every item loads on the same single dimension. In the following example, the responses are generated from two-dimensional latent traits that have a correlation of $0.8$, and we treat them as one dimension. Note that the truncated lasso ($L_1$) penalty becomes lasso penalty when `tau` takes `Inf`. In the following example, DIF detection results are ordered by pairs of groups and DIF parameters are flagged by `X`.

``` r
with(D2PL_data, D2PL_pair_em(data, group, Lambda0 = seq(1, 1.5, by = 0.1), Tau = c(Inf, seq(0.002, 0.01, by = 0.002)), verbose = FALSE))
#>        1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
#> 1,2:a1                                                   
#> 1,2:b  X X   X X                                         
#> 1,3:a1       X                                           
#> 1,3:b  X X X X X X                                       
#> 2,3:a1       X                                           
#> 2,3:b  X   X X X X                                       
#> * lambda0 = 1.2, tau = 0.004
```

If `D2PL_pair_em` does not fit the need, we recommend `D2PL_em` for low-dimensional cases (e.g., $D\leq 3$) because it is more accurate; `D2PL_gvem` is recommend for high-dimensional cases and/or fast estimation. Both functions require item responses, loading indicator, and group membership. Besides, estimation method (`method`) and tuning parameter vector (`Lambda0`) are the two most important arguments. `EMM` and `IWGVEMM` are the default choices and are recommended for `D2PL_em` and `D2PL_gvem` respectively because these methods are more accurate. Specifically, `IWGVEMM` has an additional importance sampling step after the GVEM estimation. We do not recommend `D2PL_lrt` because it is time-consuming. Users are referred to @wang2023 for `D2PL_em` and @lyu2025 for `D2PL_gvem`. In the example below, results are ordered by groups and group 1 is the reference group. DIF parameters are flagged by `X`, indicating that the item parameter of this group is different from that of group 1.

``` r
result <- with(D2PL_data, D2PL_gvem(data, model, group, method = 'IWGVEMM', Lambda0 = seq(0.2, 0.7, by = 0.1), verbose = F))
result
#>      1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
#> 2:a1                                                   
#> 2:a2                                                   
#> 2:b          X                                         
#> 3:a1                                                   
#> 3:a2       X                                           
#> 3:b  X X X X X X                                       
#> * lambda0 = 0.5
```

By default, both `D2PL_em` and `D2PL_pair_em` choose the best tuning parameters using the Bayesian information criterion (BIC), while `D2PL_gvem` uses the generalized information criterion (GIC) with $c=1$. In the example above 0.5 is chosen, but AIC, BIC, or GIC with other values of $c$ can also be used by specifying `"AIC"`, `"BIC"`, or the value of $c$ in functions `coef`, `print` and `summary`. We suggest $c$ be from $0$ to $1$, where larger values lead to lower true and false positive rates.

``` r
summary(result, 0.5)
#>    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
#> a1         X                                         
#> a2       X   X                                       
#> b  X X X X X X   X    X                              
#> * lambda0 = 0.3
print(result, 'AIC')
#> Warning in check.vemirt_DIF(all, fit, "lambda0"): Optimal lambda0 may be less than 0.2.
#>      1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
#> 2:a1                                                   
#> 2:a2                                                   
#> 2:b  X X   X X   X      X        X     X  X     X      
#> 3:a1     X   X       X                                 
#> 3:a2   X   X   X                                       
#> 3:b  X X X X X X   X X  X                       X     X
#> * lambda0 = 0.2
```
The message warns us that the optimal tuning parameter for AIC may be out of the range specified for estimation. Users should specify a wider range for the corresponding argument if the current information criterion is used. Finally, other parameter estimates can be obtained too:

``` r
str(coef(result, 'BIC'))
#> List of 17
#>  $ lambda0: num 0.3
#>  $ lambda : num 11.6
#>  $ niter  : num [1:2] 109 104
#>  $ ll     : num -11570
#>  $ l0     : int 14
#>  $ SIGMA  : num [1:1500, 1:2, 1:2] 0.103 0.124 0.116 0.103 0.13 ...
#>  $ MU     : num [1:1500, 1:2] 0.00366 -1.21956 -0.86532 0.05182 -1.44339 ...
#>  $ Sigma  : num [1:3, 1:2, 1:2] 1 0.991 1.034 0.829 0.823 ...
#>  $ Mu     : num [1:3, 1:2] 0 -0.0405 0.0701 0 -0.1188 ...
#>  $ a      : num [1:20, 1:2] 2.28 0 2.09 0 1.95 ...
#>  $ b      : num [1:20] 1.0022 0.649 0.0387 0.8109 -0.1485 ...
#>  $ gamma  : num [1:3, 1:20, 1:2] 0 0 0 0 0 0 0 0 0 0 ...
#>  $ beta   : num [1:3, 1:20] 0 0 0.853 0 0.608 ...
#>  $ RMSE   : num 0.0173
#>  $ AIC    : num 23168
#>  $ BIC    : num 23242
#>  $ GIC    : num 23343
```


# Package Evaluation
Here we show two examples on how to test the `VEMIRT` package by simulating data, estimating the model using `VEMIRT`, and then checking accuracy.

``` r
library(abind)
library(mvtnorm)
```

## Confirmatory 2PL Model

``` r
set.seed(1)
Sigma <- matrix(c(1, 0.85, 0.85, 1), 2)
J <- 10
N <- 1000
model <- cbind(rep(1:0, J / 2), rep(0:1, J / 2))
a <- matrix(runif(J * 2, 1, 3), ncol = 2) * model
b <- rnorm(J)
theta <- rmvnorm(N, rep(0, 2), Sigma)
data <- t(matrix(rbinom(N * J, 1, plogis(a %*% t(theta) - b)), nrow = J))

result.gvem <- C2PL_gvem(data, model)
result.iw <- C2PL_iw(data, result.gvem)
result.iw2 <- C2PL_iw2(data, model)

rmse <- function(x, y) {
  sqrt(mean((x - y) ^ 2))
}
c(a = rmse(a[model == 1], coef(result.gvem)[, 1:2][model == 1]), b = rmse(b, coef(result.gvem)$b))
#>         a         b 
#> 0.6039514 0.1318309
c(a = rmse(a[model == 1], coef(result.iw)[, 1:2][model == 1]), b = rmse(b, coef(result.iw)$b))
#>          a          b 
#> 0.40411993 0.09705373
c(a = rmse(a[model == 1], coef(result.iw2)$a[model == 1]), b = rmse(b, coef(result.iw2)$b))
#>         a         b 
#> 0.1370900 0.0771889
```


## DIF 2PL Model

``` r
set.seed(1)
Sigma <- matrix(c(1, 0.85, 0.85, 1), 2)
J <- 10
j <- J * 0.4
n <- 300
group <- rep(1:3, each = n)
model <- cbind(rep(1:0, J / 2), rep(0:1, J / 2))
a <- matrix(runif(J * 2, 1, 3), ncol = 2) * model
a <- unname(abind(a, a, a, along = 0))
a[-1, 1:(j / 2), ] <- a[-1, 1:(j / 2), ] + c(0.5, 1)
a[-1, (j / 2 + 1):j, ] <- a[-1, (j / 2 + 1):j, ] - c(0.5, 1)
a[-1, , ] <- a[-1, , ] * abind(model, model, along = 0)
b <- rnorm(J)
b <- unname(rbind(b, b, b))
b[-1, 1:(j / 2)] <- b[-1, 1:(j / 2)] - c(0.5, 1)
b[-1, (j / 2 + 1):j] <- b[-1, (j / 2 + 1):j] + c(0.5, 1)
theta <- rmvnorm(n * 3, rep(0, 2), Sigma)
data <- t(sapply(1:(n * 3), function(n) {
  rbinom(J, 1, plogis(a[group[n], , ] %*% theta[n, ] - b[group[n], ]))
}))

result.iw <- D2PL_gvem(data, model, group, verbose = F)
result.iw.gic_0.3 <- summary(result.iw, 0.3)
result.iw.gic_1 <- summary(result.iw, 1)
result.iw.bic <- summary(result.iw, 'BIC')

count <- function(j, result) {
  pos <- colSums(result) > 0
  c(`True Positive` = mean(pos[1:j]), `False Positive` = mean(pos[-(1:j)]))
}
count(j, coef(result.iw.gic_0.3))
#>  True Positive False Positive 
#>            1.0            0.5
count(j, coef(result.iw.gic_1))
#>  True Positive False Positive 
#>           0.75           0.00
count(j, coef(result.iw.bic))
#>  True Positive False Positive 
#>      1.0000000      0.1666667
```


# References
<div id = "refs"></div>
